{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "direction": "ltr"
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "from TorchEASE.src.main.EASE import TorchEASE\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/msd/pro_sg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(join(data_path, f\"train.csv\"))\n",
    "\n",
    "val_df_tr = pd.read_csv(join(data_path, f\"validation_tr.csv\"))\n",
    "val_df_te = pd.read_csv(join(data_path, f\"validation_te.csv\"))\n",
    "\n",
    "test_df_tr = pd.read_csv(join(data_path, f\"test_tr.csv\"))\n",
    "test_df_te = pd.read_csv(join(data_path, f\"test_te.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df[:int(1e5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>sid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44118</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44118</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44118</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44118</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     uid  sid\n",
       "0  44118    0\n",
       "1  44118    1\n",
       "2  44118    2\n",
       "3  44118    3\n",
       "4  44118    4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-30 10:59:37 [INFO] notebook - Building user + item lookup\n",
      "2021-06-30 10:59:38 [INFO] notebook - Building item hashmap\n",
      "2021-06-30 10:59:46 [INFO] notebook - User + item lookup complete\n",
      "2021-06-30 10:59:47 [INFO] notebook - Sparse data built\n"
     ]
    }
   ],
   "source": [
    "te_implicit = TorchEASE(train_df, user_col=\"uid\", item_col=\"sid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-30 10:59:47 [INFO] notebook - Building G Matrix\n",
      "2021-06-30 11:57:28 [INFO] notebook - Building B matrix\n",
      "3468.7566916942596\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "te_implicit.fit()\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4752db65ebab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_implicit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/recsys/bagging_ease/bagging-ease/src/evaluate.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(ease_model, eval_tr, eval_te, k)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mease_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mease_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0muid_to_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/recsys/bagging_ease/bagging-ease/TorchEASE/src/main/EASE.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, df, k)\u001b[0m\n\u001b[1;32m    101\u001b[0m                                                    interaction_size).to_dense()\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_interactions\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "a=evaluate(te_implicit, val_df_tr, val_df_te, k=20)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at(predictions, labels, k=10, assume_unique=True):\n",
    "    \"\"\"Compute the normalized discounted cumulative gain at K.\n",
    "    Compute the average NDCG value of all the queries, truncated at ranking\n",
    "    position k. The discounted cumulative gain at position k is computed as:\n",
    "        sum,,i=1,,^k^ (2^{relevance of ''i''th item}^ - 1) / log(i + 1)\n",
    "    and the NDCG is obtained by dividing the DCG value on the ground truth set.\n",
    "    In the current implementation, the relevance value is binary.\n",
    "    If a query has an empty ground truth set, zero will be used as\n",
    "    NDCG together with a warning.\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : array-like, shape=(n_predictions,)\n",
    "        The prediction array. The items that were predicted, in descending\n",
    "        order of relevance.\n",
    "    labels : array-like, shape=(n_ratings,)\n",
    "        The labels (positively-rated items).\n",
    "    k : int, optional (default=10)\n",
    "        The rank at which to measure the NDCG.\n",
    "    assume_unique : bool, optional (default=True)\n",
    "        Whether to assume the items in the labels and predictions are each\n",
    "        unique. That is, the same item is not predicted multiple times or\n",
    "        rated multiple times.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # predictions for 3 users\n",
    "    >>> preds = [[1, 6, 2, 7, 8, 3, 9, 10, 4, 5],\n",
    "    ...          [4, 1, 5, 6, 2, 7, 3, 8, 9, 10],\n",
    "    ...          [1, 2, 3, 4, 5]]\n",
    "    >>> # labels for the 3 users\n",
    "    >>> labels = [[1, 2, 3, 4, 5], [1, 2, 3], []]\n",
    "    >>> ndcg_at(preds, labels, 3)\n",
    "    0.3333333432674408\n",
    "    >>> ndcg_at(preds, labels, 10)\n",
    "    0.48791273434956867\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] K. Jarvelin and J. Kekalainen, \"IR evaluation methods for\n",
    "           retrieving highly relevant documents.\"\n",
    "    \"\"\"\n",
    "    # validate K\n",
    "    _require_positive_k(k)\n",
    "\n",
    "    def _inner_ndcg(pred, lab):\n",
    "        if lab.shape[0]:\n",
    "            # if we do NOT assume uniqueness, the set is a bit different here\n",
    "            if not assume_unique:\n",
    "                lab = np.unique(lab)\n",
    "\n",
    "            n_lab = lab.shape[0]\n",
    "            n_pred = pred.shape[0]\n",
    "            n = min(max(n_pred, n_lab), k)  # min(min(p, l), k)?\n",
    "\n",
    "            # similar to mean_avg_prcsn, we need an arange, but this time +2\n",
    "            # since python is zero-indexed, and the denom typically needs +1.\n",
    "            # Also need the log base2...\n",
    "            arange = np.arange(n, dtype=np.float32)  # length n\n",
    "\n",
    "            # since we are only interested in the arange up to n_pred, truncate\n",
    "            # if necessary\n",
    "            arange = arange[:n_pred]\n",
    "            denom = np.log2(arange + 2.)  # length n\n",
    "            gains = 1. / denom  # length n\n",
    "\n",
    "            # compute the gains where the prediction is present in the labels\n",
    "            dcg_mask = np.in1d(pred[:n], lab, assume_unique=assume_unique)\n",
    "            dcg = gains[dcg_mask].sum()\n",
    "\n",
    "            # the max DCG is sum of gains where the index < the label set size\n",
    "            max_dcg = gains[arange < n_lab].sum()\n",
    "            return dcg / max_dcg\n",
    "\n",
    "        else:\n",
    "            return _warn_for_empty_labels()\n",
    "\n",
    "    return _mean_ranking_metric(predictions, labels, _inner_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_val_users = val_df_tr.uid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_idxes = val_df_tr.uid.isin(unique_val_users[:1])\n",
    "play_val = val_df_tr[curr_idxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18633031845092773\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "predictions = te_implicit.predict(play_val, k=100)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_to_prediction = dict(zip(play_val.uid.unique(), predictions.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = val_df_te.uid.isin(unique_val_users[:1])\n",
    "play_val_te = val_df_te[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_with_true = play_val_te.groupby(\"uid\").agg({'sid':list})\n",
    "preds_with_true = preds_with_true.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_with_true['preds'] = preds_with_true.apply(lambda x:uid_to_prediction.get(x.uid), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>sid</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512467</td>\n",
       "      <td>[195, 18094, 13873, 1519, 1972, 5705, 7414, 74...</td>\n",
       "      <td>[2199, 103, 351, 1519, 223, 1534, 2751, 2984, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      uid                                                sid  \\\n",
       "0  512467  [195, 18094, 13873, 1519, 1972, 5705, 7414, 74...   \n",
       "\n",
       "                                               preds  \n",
       "0  [2199, 103, 351, 1519, 223, 1534, 2751, 2984, ...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_with_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_with_true.apply(lambda x:recall_at_k(x.sid, x.preds, 50), axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = val_df_te[val_df_te.uid == unique_val_users[0]].groupby('uid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Liorhi Env",
   "language": "python",
   "name": "liorhi_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
